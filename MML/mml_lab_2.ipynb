{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e3506ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aae12bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49ca2027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained ResNet50\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "resnet = torch.nn.Sequential(*(list(resnet.children())[:-1]))  # remove final FC layer\n",
    "resnet.eval().to(device)\n",
    "\n",
    "# Load pretrained BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert.eval().to(device)\n",
    "\n",
    "# Image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96f463a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_features(img_path):\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    img_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = resnet(img_tensor)  # [1, 2048, 1, 1]\n",
    "    return features.flatten(1).cpu().numpy()  # [1, 2048]\n",
    "\n",
    "def extract_text_features(caption):\n",
    "    inputs = tokenizer(caption, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert(**inputs)\n",
    "    return outputs.last_hidden_state[:,0,:].cpu().numpy()  # [1, 768]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c61d142f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_features_npy(coco, img_dir, save_dir=\"features\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    img_ids = coco.getImgIds()\n",
    "    print(f\"Extracting features for {len(img_ids)} images...\")\n",
    "\n",
    "    all_image_features = []\n",
    "    all_caption_features = []\n",
    "\n",
    "    for img_id in tqdm(img_ids):\n",
    "        img_info = coco.loadImgs(img_id)[0]\n",
    "        img_path = f\"{img_dir}/{img_info['file_name']}\"\n",
    "\n",
    "        try:\n",
    "            # ---- IMAGE FEATURES ----\n",
    "            img_features = extract_image_features(img_path)\n",
    "            all_image_features.append(img_features.squeeze(0))\n",
    "\n",
    "            # ---- CAPTION FEATURES ----\n",
    "            ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "            anns = coco.loadAnns(ann_ids)\n",
    "            captions = [ann['caption'] for ann in anns if \"caption\" in ann]\n",
    "\n",
    "            cap_features = []\n",
    "            for cap in captions:\n",
    "                cap_feat = extract_text_features(cap)\n",
    "                cap_features.append(cap_feat.squeeze(0))\n",
    "            all_caption_features.append(np.array(cap_features))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {img_id} due to error: {e}\")\n",
    "\n",
    "    all_image_features = np.array(all_image_features)\n",
    "\n",
    "    np.save(os.path.join(save_dir, \"images.npy\"), all_image_features)\n",
    "    np.save(os.path.join(save_dir, \"captions.npy\"), np.array(all_caption_features, dtype=object))\n",
    "\n",
    "    print(f\"Saved image features to {save_dir}/images.npy\")\n",
    "    print(f\"Saved caption features to {save_dir}/captions.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31b766de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.38s)\n",
      "creating index...\n",
      "index created!\n",
      "Extracting features for 5000 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:39<00:00, 22.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image features to /home/BTECH_7TH_SEM/Desktop/MML-RL-and-NLP/MML/coco_features/images.npy\n",
      "Saved caption features to /home/BTECH_7TH_SEM/Desktop/MML-RL-and-NLP/MML/coco_features/captions.npy\n"
     ]
    }
   ],
   "source": [
    "# Example paths (change these to your setup)\n",
    "annFile = \"/home/BTECH_7TH_SEM/MS-COCO/annotations_trainval2017/annotations/captions_val2017.json\"\n",
    "img_dir = \"/home/BTECH_7TH_SEM/MS-COCO/val2017\"\n",
    "save_dir = \"/home/BTECH_7TH_SEM/Desktop/MML-RL-and-NLP/MML/coco_features\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "coco = COCO(annFile)\n",
    "extract_all_features_npy(coco, img_dir, save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3477779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (5000, 2048)\n",
      "Number of caption sets: 5000\n",
      "Captions for first image shape: (5, 768)\n"
     ]
    }
   ],
   "source": [
    "# Load saved features\n",
    "images = np.load(\"coco_features/images.npy\")\n",
    "captions = np.load(\"coco_features/captions.npy\", allow_pickle=True)\n",
    "\n",
    "print(\"Images shape:\", images.shape)\n",
    "print(\"Number of caption sets:\", len(captions))\n",
    "print(\"Captions for first image shape:\", captions[0].shape if len(captions) > 0 else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365279f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mml-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
